{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cc068e35",
   "metadata": {},
   "outputs": [],
   "source": [
    "import PyPDF2\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    \"\"\"\n",
    "    Extracts all text from a given PDF file.\n",
    "\n",
    "    Args:\n",
    "        pdf_path (str): The path to the PDF file.\n",
    "\n",
    "    Returns:\n",
    "        str: A single string containing all extracted text from the PDF.\n",
    "    \"\"\"\n",
    "    text = \"\"\n",
    "    try:\n",
    "        with open(pdf_path, 'rb') as file:\n",
    "            reader = PyPDF2.PdfReader(file)\n",
    "            for page_num in range(len(reader.pages)):\n",
    "                page = reader.pages[page_num]\n",
    "                text += page.extract_text()\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: The file at {pdf_path} was not found.\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while processing {pdf_path}: {e}\")\n",
    "        return None\n",
    "    return text\n",
    "\n",
    "# --- Implementation Instructions ---\n",
    "\n",
    "# 1. Install PyPDF2:\n",
    "#    If you don't have PyPDF2 installed, open your terminal or command prompt and run:\n",
    "#    pip install PyPDF2\n",
    "\n",
    "# 2. Save the code:\n",
    "#    Save the above Python code as a .py file (e.g., 'pdf_processor.py').\n",
    "\n",
    "# 3. Place your PDF files:\n",
    "#    Make sure 'AmazonElasticComputeCloud-DeveloperGuide-ec2-dg.pdf'\n",
    "#    and 'Amazon_SageMaker_FAQs.pdf' are in the same directory as your Python script,\n",
    "#    or provide the full path to your PDF files.\n",
    "\n",
    "# 4. Use the function to extract text:\n",
    "#    You can call the function like this:\n",
    "#    ec2_pdf_path = \"AmazonElasticComputeCloud-DeveloperGuide-ec2-dg.pdf\"\n",
    "#    sagemaker_pdf_path = \"Amazon_SageMaker_FAQs.pdf\"\n",
    "#\n",
    "#    ec2_text = extract_text_from_pdf(ec2_pdf_path)\n",
    "#    sagemaker_text = extract_text_from_pdf(sagemaker_pdf_path)\n",
    "#\n",
    "#    if ec2_text:\n",
    "#        print(f\"Extracted text from {ec2_pdf_path} (first 500 chars):\\n{ec2_text[:500]}...\")\n",
    "#    if sagemaker_text:\n",
    "#        print(f\"\\nExtracted text from {sagemaker_pdf_path} (first 500 chars):\\n{sagemaker_text[:500]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "34be95d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted text from ./AmazonElasticComputeCloud-DeveloperGuide-ec2-dg.pdf (first 500 chars):\n",
      "DeveloperGuideAmazonElasticComputeCloud\n",
      "Copyright©2025AmazonWebServices,Inc.and/oritsaﬃliates.Allrightsreserved.AmazonElasticComputeCloudDeveloperGuideAmazonElasticComputeCloud:DeveloperGuideCopyright©2025AmazonWebServices,Inc.and/oritsaﬃliates.Allrightsreserved.Amazon'strademarksandtradedressmaynotbeusedinconnectionwithanyproductorservicethatisnotAmazon's,inanymannerthatislikelytocauseconfusionamongcustomers,orinanymannerthatdisparagesordiscreditsAmazon.AllothertrademarksnotownedbyAmazonarethep...\n",
      "\n",
      "Extracted text from ./Amazon_SageMaker_FAQs.pdf (first 500 chars):\n",
      "What is Amazon SageMaker?,\"Amazon SageMaker is a fully managed service to prepare data and build, train, and deploy machine learning (ML) models for any use case with fully managed infrastructure, tools, and workflows.\"\"In which Regions is Amazon SageMaker available?\",\"For a list of the supported Amazon SageMaker AWS Regions, please visit the AWS Regional Services page. Also, for more information, see Regional endpoints in the AWS general reference guide.\"\"What is the service availability of Ama...\n"
     ]
    }
   ],
   "source": [
    "ec2_pdf_path = \"./AmazonElasticComputeCloud-DeveloperGuide-ec2-dg.pdf\"\n",
    "sagemaker_pdf_path = \"./Amazon_SageMaker_FAQs.pdf\"\n",
    "\n",
    "ec2_text = extract_text_from_pdf(ec2_pdf_path)\n",
    "sagemaker_text = extract_text_from_pdf(sagemaker_pdf_path)\n",
    "\n",
    "if ec2_text:\n",
    "    print(f\"Extracted text from {ec2_pdf_path} (first 500 chars):\\n{ec2_text[:500]}...\")\n",
    "if sagemaker_text:\n",
    "    print(f\"\\nExtracted text from {sagemaker_pdf_path} (first 500 chars):\\n{sagemaker_text[:500]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f637bd9",
   "metadata": {},
   "source": [
    "Once you have the full text from your PDFs, the next crucial step for a RAG system is chunking. Chunking involves breaking down the large text into smaller, manageable pieces. This is important because:\n",
    "\n",
    "Context Window Limits: Language models often have a limit on how much text they can process at once (their context window). Chunks ensure we stay within these limits.\n",
    "Relevance: Smaller chunks help in retrieving more relevant information. If a query matches a small, precise chunk, it's often better than retrieving a huge document where the relevant information might be diluted.\n",
    "Efficiency: Processing and embedding smaller chunks is generally more efficient.\n",
    "Good Chunking Strategy: Recursive Character Text Splitter\n",
    "A highly recommended and robust chunking strategy is using a Recursive Character Text Splitter. This method attempts to split text using a list of characters, trying them in order until the chunks are small enough. This helps to keep sentences and paragraphs together as much as possible, which preserves semantic meaning.\n",
    "\n",
    "The best Python library for this is langchain-text-splitters (part of the LangChain ecosystem, which is excellent for building RAG systems).\n",
    "\n",
    "Overview of the Solution\n",
    "We will:\n",
    "\n",
    "Install langchain-text-splitters: If you don't have it already.\n",
    "Define a RecursiveCharacterTextSplitter: We'll specify parameters like chunk_size (the maximum size of each chunk) and chunk_overlap (how much overlap there should be between consecutive chunks to maintain context).\n",
    "Apply the splitter to your extracted text: This will generate a list of text chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "83ec80ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "def chunk_text(text, chunk_size=1000, chunk_overlap=200):\n",
    "    \"\"\"\n",
    "    Splits a given text into smaller, overlapping chunks using RecursiveCharacterTextSplitter.\n",
    "\n",
    "    Args:\n",
    "        text (str): The input text to be chunked.\n",
    "        chunk_size (int): The maximum number of characters in each chunk.\n",
    "        chunk_overlap (int): The number of characters to overlap between consecutive chunks.\n",
    "\n",
    "    Returns:\n",
    "        list[str]: A list of text chunks.\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str) or not text:\n",
    "        print(\"Error: Input text must be a non-empty string.\")\n",
    "        return []\n",
    "\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "        length_function=len,  # Use character length\n",
    "        add_start_index=True, # Add start index to metadata (useful for debugging/tracking)\n",
    "    )\n",
    "    chunks = text_splitter.split_text(text)\n",
    "    return chunks\n",
    "\n",
    "# --- Implementation Instructions ---\n",
    "\n",
    "# 1. Install the necessary library:\n",
    "#    If you haven't already, open your terminal or anaconda prompt and run:\n",
    "#    pip install langchain-text-splitters\n",
    "\n",
    "# 2. Ensure you have your extracted text:\n",
    "#    Before running this, make sure you have executed the previous steps and have\n",
    "#    your PDF text extracted into variables like 'ec2_text' and 'sagemaker_text'.\n",
    "#    For example:\n",
    "#    # ec2_text = extract_text_from_pdf(\"AmazonElasticComputeCloud-DeveloperGuide-ec2-dg.pdf\")\n",
    "#    # sagemaker_text = extract_text_from_pdf(\"Amazon_SageMaker_FAQs.pdf\")\n",
    "\n",
    "# 3. Apply the chunking function:\n",
    "#    Now, you can call the 'chunk_text' function with your extracted PDF texts:\n",
    "\n",
    "#    # Example usage (assuming ec2_text and sagemaker_text are already defined from previous step):\n",
    "#    # if ec2_text: # Check if extraction was successful\n",
    "#    #     ec2_chunks = chunk_text(ec2_text, chunk_size=1000, chunk_overlap=200)\n",
    "#    #     print(f\"\\nNumber of chunks from EC2 PDF: {len(ec2_chunks)}\")\n",
    "#    #     print(f\"First 2 EC2 chunks:\\n{ec2_chunks[0]}\\n---\\n{ec2_chunks[1]}...\")\n",
    "#\n",
    "#    # if sagemaker_text: # Check if extraction was successful\n",
    "#    #     sagemaker_chunks = chunk_text(sagemaker_text, chunk_size=1000, chunk_overlap=200)\n",
    "#    #     print(f\"\\nNumber of chunks from SageMaker PDF: {len(sagemaker_chunks)}\")\n",
    "#    #     print(f\"First 2 SageMaker chunks:\\n{sagemaker_chunks[0]}\\n---\\n{sagemaker_chunks[1]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e05bac36",
   "metadata": {},
   "outputs": [],
   "source": [
    "ec2_chunks = chunk_text(ec2_text, chunk_size=1000, chunk_overlap=200)\n",
    "sagemaker_chunks = chunk_text(sagemaker_text, chunk_size=1000, chunk_overlap=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2f9283fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "1662\n",
      "----------------------------------------------\n",
      "<class 'list'>\n",
      "97\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "pprint(type(ec2_chunks))\n",
    "pprint(len(ec2_chunks))\n",
    "print(\"----------------------------------------------\")\n",
    "pprint(type(sagemaker_chunks))\n",
    "pprint(len(sagemaker_chunks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3d126bff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/anuragmathur/repos/GitHub/GenAIEngineering-Outskill/MyWork/AssignmentRepo/outskill-assignments/.venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Sentence Transformer model...\n",
      "Model loaded.\n",
      "Initializing Qdrant client...\n",
      "<qdrant_client.qdrant_client.QdrantClient object at 0x323e5f230>\n",
      "title='qdrant - vector search engine' version='1.14.1' commit='530430fac2a3ca872504f276d2c91a5c91f43fa0'\n",
      "Qdrant client initialized.\n",
      "Embedding vector size: 384\n",
      "Collection 'my_rag_documents' already exists.\n",
      "Generating embeddings and preparing points for Qdrant...\n",
      "Processing EC2 chunks...\n",
      "Processing SageMaker chunks...\n",
      "Successfully uploaded 1759 chunks to Qdrant collection 'my_rag_documents'.\n"
     ]
    }
   ],
   "source": [
    "# Install necessary libraries (if you haven't already)\n",
    "# !pip install sentence-transformers qdrant-client\n",
    "\n",
    "from qdrant_client import QdrantClient, models\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import uuid # To generate unique IDs for our chunks\n",
    "\n",
    "# --- 1. Load the Embedding Model ---\n",
    "# This will download the model the first time it's run\n",
    "print(\"Loading Sentence Transformer model...\")\n",
    "embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "print(\"Model loaded.\")\n",
    "\n",
    "def get_embedding(text):\n",
    "    \"\"\"\n",
    "    Generates an embedding for a given text using the loaded model.\n",
    "\n",
    "    Args:\n",
    "        text (str): The input text.\n",
    "\n",
    "    Returns:\n",
    "        list[float]: A list representing the embedding vector.\n",
    "    \"\"\"\n",
    "    # Encode the text to get the embedding vector\n",
    "    return embedding_model.encode(text).tolist()\n",
    "\n",
    "# --- 2. Start Qdrant (External Step) ---\n",
    "# Before running the Python code below, you need to start Qdrant.\n",
    "# Open your terminal or command prompt and run the following Docker command:\n",
    "# docker run -p 6333:6333 -p 6334:6334 \\\n",
    "#     -v $(pwd)/qdrant_storage:/qdrant/storage \\\n",
    "#     qdrant/qdrant\n",
    "\n",
    "# Explanation of the Docker command:\n",
    "# -p 6333:6333: Maps the default gRPC port (6333) to your local machine.\n",
    "# -p 6334:6334: Maps the REST API port (6334) to your local machine.\n",
    "# -v $(pwd)/qdrant_storage:/qdrant/storage: Creates a persistent volume.\n",
    "#   This means your data will be saved in a 'qdrant_storage' folder in your current directory,\n",
    "#   so it won't be lost if you stop and restart the Docker container.\n",
    "# qdrant/qdrant: The official Qdrant Docker image.\n",
    "\n",
    "# --- 3. Initialize Qdrant Client ---\n",
    "print(\"Initializing Qdrant client...\")\n",
    "qdrant_client = QdrantClient(host=\"localhost\", port=6333) # Connect to the local Qdrant instance\n",
    "print(qdrant_client)\n",
    "print(qdrant_client.info()) # Print Qdrant server info to confirm connection\n",
    "print(\"Qdrant client initialized.\")\n",
    "\n",
    "# ... (embedding_model and qdrant_client setup as before) ...\n",
    "\n",
    "# Assuming ec2_chunks and sagemaker_chunks are lists of strings from your chunking step\n",
    "# Make sure these variables are populated from the previous step in your notebook.\n",
    "\n",
    "# --- 4. Define Collection Name and Vector Size ---\n",
    "collection_name = \"my_rag_documents\"\n",
    "# The vector size must match the output dimension of your embedding model.\n",
    "# 'all-MiniLM-L6-v2' produces 384-dimensional embeddings.\n",
    "vector_size = embedding_model.get_sentence_embedding_dimension() # Gets dimension from model\n",
    "\n",
    "print(f\"Embedding vector size: {vector_size}\") # You should see this line in your output\n",
    "\n",
    "# --- 5. Create Qdrant Collection (This is the critical part!) ---\n",
    "try:\n",
    "    # Check if the collection already exists\n",
    "    qdrant_client.get_collection(collection_name=collection_name)\n",
    "    print(f\"Collection '{collection_name}' already exists.\") # You would see this if it already existed\n",
    "except Exception:\n",
    "    # If not, create it (or recreate if it existed but was cleared)\n",
    "    print(f\"Creating collection '{collection_name}'...\") # You MUST see this line in your output if it's being created\n",
    "    qdrant_client.recreate_collection(\n",
    "        collection_name=collection_name,\n",
    "        vectors_config=models.VectorParams(size=vector_size, distance=models.Distance.COSINE),\n",
    "        # Cosine distance is standard for many embedding models like Sentence Transformers\n",
    "    )\n",
    "    print(f\"Collection '{collection_name}' created.\") # You MUST see this line if creation was successful\n",
    "\n",
    "# ... (Collection creation as before) ...\n",
    "\n",
    "points = []\n",
    "print(\"Generating embeddings and preparing points for Qdrant...\")\n",
    "\n",
    "# Process EC2 chunks\n",
    "print(\"Processing EC2 chunks...\")\n",
    "for i, chunk in enumerate(ec2_chunks):\n",
    "    if not chunk.strip(): # Skip empty chunks\n",
    "        continue\n",
    "    \n",
    "    embedding = get_embedding(chunk)\n",
    "    point_id = str(uuid.uuid4()) # Generate a unique ID for each point\n",
    "\n",
    "    payload = {\n",
    "        \"text\": chunk,\n",
    "        \"source\": \"AmazonElasticComputeCloud-DeveloperGuide-ec2-dg.pdf\", # Actual source filename\n",
    "        \"chunk_index\": i # Optional: helps identify chunk order within its original document\n",
    "    }\n",
    "\n",
    "    points.append(\n",
    "        models.PointStruct(\n",
    "            id=point_id,\n",
    "            vector=embedding,\n",
    "            payload=payload\n",
    "        )\n",
    "    )\n",
    "\n",
    "# Process SageMaker chunks\n",
    "print(\"Processing SageMaker chunks...\")\n",
    "for i, chunk in enumerate(sagemaker_chunks):\n",
    "    if not chunk.strip(): # Skip empty chunks\n",
    "        continue\n",
    "    \n",
    "    embedding = get_embedding(chunk)\n",
    "    point_id = str(uuid.uuid4())\n",
    "\n",
    "    payload = {\n",
    "        \"text\": chunk,\n",
    "        \"source\": \"Amazon_SageMaker_FAQs.pdf\", # Actual source filename\n",
    "        \"chunk_index\": i\n",
    "    }\n",
    "\n",
    "    points.append(\n",
    "        models.PointStruct(\n",
    "            id=point_id,\n",
    "            vector=embedding,\n",
    "            payload=payload\n",
    "        )\n",
    "    )\n",
    "\n",
    "if points:\n",
    "    qdrant_client.upsert(\n",
    "        collection_name=collection_name,\n",
    "        wait=True,\n",
    "        points=points\n",
    "    )\n",
    "    print(f\"Successfully uploaded {len(points)} chunks to Qdrant collection '{collection_name}'.\")\n",
    "else:\n",
    "    print(\"No chunks to upload or all chunks were empty.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f4875e4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Sentence Transformer model for retrieval...\n",
      "Retrieval model loaded.\n",
      "Initializing Qdrant client for retrieval...\n",
      "Qdrant client initialized for retrieval.\n"
     ]
    }
   ],
   "source": [
    "from qdrant_client import QdrantClient\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# --- 1. Load the Embedding Model (Same as used for indexing!) ---\n",
    "# Ensure this matches the model used to embed your document chunks.\n",
    "print(\"Loading Sentence Transformer model for retrieval...\")\n",
    "retrieval_embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "print(\"Retrieval model loaded.\")\n",
    "\n",
    "def get_query_embedding(query_text):\n",
    "    \"\"\"\n",
    "    Generates an embedding for a given query text using the loaded model.\n",
    "\n",
    "    Args:\n",
    "        query_text (str): The user's input query.\n",
    "\n",
    "    Returns:\n",
    "        list[float]: A list representing the query's embedding vector.\n",
    "    \"\"\"\n",
    "    return retrieval_embedding_model.encode(query_text).tolist()\n",
    "\n",
    "# --- 2. Initialize Qdrant Client (Same as used for indexing!) ---\n",
    "print(\"Initializing Qdrant client for retrieval...\")\n",
    "retrieval_qdrant_client = QdrantClient(host=\"localhost\", port=6333) # Connect to the local Qdrant instance\n",
    "print(\"Qdrant client initialized for retrieval.\")\n",
    "\n",
    "# --- 3. Define Collection Name (Same as used for indexing!) ---\n",
    "collection_name = \"my_rag_documents\"\n",
    "\n",
    "def retrieve_context(query, top_k=3):\n",
    "    \"\"\"\n",
    "    Retrieves the most relevant text chunks from Qdrant for a given query.\n",
    "\n",
    "    Args:\n",
    "        query (str): The user's input question.\n",
    "        top_k (int): The number of top similar chunks to retrieve.\n",
    "\n",
    "    Returns:\n",
    "        list[dict]: A list of dictionaries, each containing the 'text' of a retrieved chunk\n",
    "                    and potentially other payload information like 'source'.\n",
    "    \"\"\"\n",
    "    print(f\"\\nRetrieving context for query: '{query}'\")\n",
    "    query_embedding = get_query_embedding(query)\n",
    "\n",
    "    try:\n",
    "        search_result = retrieval_qdrant_client.search(\n",
    "            collection_name=collection_name,\n",
    "            query_vector=query_embedding,\n",
    "            limit=top_k,\n",
    "            # We want to retrieve the actual text content (payload)\n",
    "            # You can select specific fields if you don't need the whole payload\n",
    "            # or set to False if you only need IDs/scores.\n",
    "            with_payload=True\n",
    "        )\n",
    "        print(f\"Retrieved {len(search_result)} chunks from Qdrant.\")\n",
    "        print(\"Search results:\" + str(search_result))\n",
    "        print(\"+++++++++++++++++++++++++++++++++++++\")\n",
    "\n",
    "        context_chunks = []\n",
    "        for hit in search_result:\n",
    "            print(hit)\n",
    "            print(\"-------------------------------\")\n",
    "            print(f\"  Retrieved chunk (score: {hit.score:.4f}, source: {hit.payload.get('source', 'N/A')}):\")\n",
    "            # Qdrant's payload is a dictionary, so access 'text'\n",
    "            chunk_text = hit.payload.get('text', 'No text found in payload')\n",
    "            print(f\"    {chunk_text[:150]}...\") # Print first 150 chars for preview\n",
    "            \n",
    "            context_chunks.append(hit.payload) # Append the entire payload for flexibility\n",
    "        \n",
    "        return context_chunks\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during Qdrant search: {e}\")\n",
    "        return []\n",
    "\n",
    "# --- Implementation Instructions ---\n",
    "\n",
    "# 1. Ensure Qdrant is still running:\n",
    "#    The Docker container you started in the previous step needs to be active.\n",
    "#    (i.e., `docker run -p 6333:6333 ... qdrant/qdrant` is still running in a terminal)\n",
    "\n",
    "# 2. Run the code:\n",
    "#    Copy this code into a new cell in your Python notebook and execute it.\n",
    "#    This will define the `get_query_embedding` and `retrieve_context` functions.\n",
    "\n",
    "# 3. Test the retrieval:\n",
    "#    Now you can test it with some sample queries:\n",
    "\n",
    "#    query1 = \"What is Amazon EC2?\"\n",
    "#    retrieved_data1 = retrieve_context(query1, top_k=2)\n",
    "#    # The 'retrieved_data1' list will contain dictionaries with 'text' and 'source'\n",
    "\n",
    "#    query2 = \"How does SageMaker help with machine learning?\"\n",
    "#    retrieved_data2 = retrieve_context(query2, top_k=2)\n",
    "\n",
    "#    # You can now see the retrieved text and potentially the source from where it came.\n",
    "#    # Example of accessing retrieved text:\n",
    "#    # if retrieved_data1:\n",
    "#    #     print(\"\\n--- Full retrieved text for Query 1 (first chunk): ---\")\n",
    "#    #     print(retrieved_data1[0]['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "70c1d57e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Retrieving context for query: 'What is Amazon EC2?'\n",
      "Retrieved 2 chunks from Qdrant.\n",
      "Search results:[ScoredPoint(id='e44b77a4-f9b5-4252-97ff-d1c9e462152c', version=0, score=0.54200673, payload={'text': 'NoteThere\\'smoreonGitHub.FindthecompleteexampleandlearnhowtosetupandrunintheAWSCodeExamplesRepository.#AssociatesanElasticIPaddresswithanAmazonElasticComputeCloud#(AmazonEC2)instance.##Prerequisites:##-TheallocationIDcorrespondingtotheElasticIPaddress.#-TheAmazonEC2instance.##@paramec2_client[Aws::EC2::Client]AninitializedEC2client.#@paramallocation_id[String]TheIDoftheallocationcorrespondingto#theElasticIPaddress.#@paraminstance_id[String]TheIDoftheinstance.#@return[String]TheassocationIDcorrespondingtotheassociationofthe#ElasticIPaddresstotheinstance.#@example#putsallocate_elastic_ip_address(#Aws::EC2::Client.new(region:\\'us-west-2\\'),#\\'eipalloc-04452e528a66279EX\\',#\\'i-033c48ef067af3dEX\\')defassociate_elastic_ip_address_with_instance(ec2_client,allocation_id,instance_id)response=ec2_client.associate_address(allocation_id:allocation_id,instance_id:instance_id)response.association_idrescueStandardError=>eputs\"ErrorassociatingElasticIPaddresswithinstance:#{e.message}\"\\'Error\\'Actions345Amazon', 'source': 'AmazonElasticComputeCloud-DeveloperGuide-ec2-dg.pdf', 'chunk_index': 474}, vector=None, shard_key=None, order_value=None), ScoredPoint(id='f8e673bf-646f-4df9-b4ad-789dc7e41fee', version=1, score=0.54200673, payload={'text': 'NoteThere\\'smoreonGitHub.FindthecompleteexampleandlearnhowtosetupandrunintheAWSCodeExamplesRepository.#AssociatesanElasticIPaddresswithanAmazonElasticComputeCloud#(AmazonEC2)instance.##Prerequisites:##-TheallocationIDcorrespondingtotheElasticIPaddress.#-TheAmazonEC2instance.##@paramec2_client[Aws::EC2::Client]AninitializedEC2client.#@paramallocation_id[String]TheIDoftheallocationcorrespondingto#theElasticIPaddress.#@paraminstance_id[String]TheIDoftheinstance.#@return[String]TheassocationIDcorrespondingtotheassociationofthe#ElasticIPaddresstotheinstance.#@example#putsallocate_elastic_ip_address(#Aws::EC2::Client.new(region:\\'us-west-2\\'),#\\'eipalloc-04452e528a66279EX\\',#\\'i-033c48ef067af3dEX\\')defassociate_elastic_ip_address_with_instance(ec2_client,allocation_id,instance_id)response=ec2_client.associate_address(allocation_id:allocation_id,instance_id:instance_id)response.association_idrescueStandardError=>eputs\"ErrorassociatingElasticIPaddresswithinstance:#{e.message}\"\\'Error\\'Actions345Amazon', 'source': 'AmazonElasticComputeCloud-DeveloperGuide-ec2-dg.pdf', 'chunk_index': 474}, vector=None, shard_key=None, order_value=None)]\n",
      "+++++++++++++++++++++++++++++++++++++\n",
      "id='e44b77a4-f9b5-4252-97ff-d1c9e462152c' version=0 score=0.54200673 payload={'text': 'NoteThere\\'smoreonGitHub.FindthecompleteexampleandlearnhowtosetupandrunintheAWSCodeExamplesRepository.#AssociatesanElasticIPaddresswithanAmazonElasticComputeCloud#(AmazonEC2)instance.##Prerequisites:##-TheallocationIDcorrespondingtotheElasticIPaddress.#-TheAmazonEC2instance.##@paramec2_client[Aws::EC2::Client]AninitializedEC2client.#@paramallocation_id[String]TheIDoftheallocationcorrespondingto#theElasticIPaddress.#@paraminstance_id[String]TheIDoftheinstance.#@return[String]TheassocationIDcorrespondingtotheassociationofthe#ElasticIPaddresstotheinstance.#@example#putsallocate_elastic_ip_address(#Aws::EC2::Client.new(region:\\'us-west-2\\'),#\\'eipalloc-04452e528a66279EX\\',#\\'i-033c48ef067af3dEX\\')defassociate_elastic_ip_address_with_instance(ec2_client,allocation_id,instance_id)response=ec2_client.associate_address(allocation_id:allocation_id,instance_id:instance_id)response.association_idrescueStandardError=>eputs\"ErrorassociatingElasticIPaddresswithinstance:#{e.message}\"\\'Error\\'Actions345Amazon', 'source': 'AmazonElasticComputeCloud-DeveloperGuide-ec2-dg.pdf', 'chunk_index': 474} vector=None shard_key=None order_value=None\n",
      "-------------------------------\n",
      "  Retrieved chunk (score: 0.5420, source: AmazonElasticComputeCloud-DeveloperGuide-ec2-dg.pdf):\n",
      "    NoteThere'smoreonGitHub.FindthecompleteexampleandlearnhowtosetupandrunintheAWSCodeExamplesRepository.#AssociatesanElasticIPaddresswithanAmazonElasticC...\n",
      "id='f8e673bf-646f-4df9-b4ad-789dc7e41fee' version=1 score=0.54200673 payload={'text': 'NoteThere\\'smoreonGitHub.FindthecompleteexampleandlearnhowtosetupandrunintheAWSCodeExamplesRepository.#AssociatesanElasticIPaddresswithanAmazonElasticComputeCloud#(AmazonEC2)instance.##Prerequisites:##-TheallocationIDcorrespondingtotheElasticIPaddress.#-TheAmazonEC2instance.##@paramec2_client[Aws::EC2::Client]AninitializedEC2client.#@paramallocation_id[String]TheIDoftheallocationcorrespondingto#theElasticIPaddress.#@paraminstance_id[String]TheIDoftheinstance.#@return[String]TheassocationIDcorrespondingtotheassociationofthe#ElasticIPaddresstotheinstance.#@example#putsallocate_elastic_ip_address(#Aws::EC2::Client.new(region:\\'us-west-2\\'),#\\'eipalloc-04452e528a66279EX\\',#\\'i-033c48ef067af3dEX\\')defassociate_elastic_ip_address_with_instance(ec2_client,allocation_id,instance_id)response=ec2_client.associate_address(allocation_id:allocation_id,instance_id:instance_id)response.association_idrescueStandardError=>eputs\"ErrorassociatingElasticIPaddresswithinstance:#{e.message}\"\\'Error\\'Actions345Amazon', 'source': 'AmazonElasticComputeCloud-DeveloperGuide-ec2-dg.pdf', 'chunk_index': 474} vector=None shard_key=None order_value=None\n",
      "-------------------------------\n",
      "  Retrieved chunk (score: 0.5420, source: AmazonElasticComputeCloud-DeveloperGuide-ec2-dg.pdf):\n",
      "    NoteThere'smoreonGitHub.FindthecompleteexampleandlearnhowtosetupandrunintheAWSCodeExamplesRepository.#AssociatesanElasticIPaddresswithanAmazonElasticC...\n",
      "\n",
      "Retrieving context for query: 'How does SageMaker help with machine learning?'\n",
      "Retrieved 2 chunks from Qdrant.\n",
      "Search results:[ScoredPoint(id='b9341eb9-a580-4808-8052-231a26d4963c', version=0, score=0.7403473, payload={'text': 'trained. These details can help determine if a particular model input has more influence than it should on overall model behavior. SageMaker Clarify also makes explanations for individual predictions available via an API.\\xa0What is Amazon SageMaker Studio?,\"Amazon SageMaker Studio provides a single, web-based visual interface where you can perform all ML development steps. SageMaker Studio gives you complete access, control, and visibility into each step required to prepare data and build, train, and deploy models. You can quickly upload data, create new notebooks, train and tune models, move back and forth between steps to adjust experiments, compare results, and deploy models to production all in one place, making you much more productive. All ML development activities including notebooks, experiment management, automatic model creation, debugging and profiling, and model drift detection can be performed within the unified SageMaker Studio visual interface.\"What is RStudio on Amazon', 'source': 'Amazon_SageMaker_FAQs.pdf', 'chunk_index': 8}, vector=None, shard_key=None, order_value=None), ScoredPoint(id='24c2b24d-dd8f-4115-b739-a702510a235f', version=1, score=0.7403473, payload={'text': 'trained. These details can help determine if a particular model input has more influence than it should on overall model behavior. SageMaker Clarify also makes explanations for individual predictions available via an API.\\xa0What is Amazon SageMaker Studio?,\"Amazon SageMaker Studio provides a single, web-based visual interface where you can perform all ML development steps. SageMaker Studio gives you complete access, control, and visibility into each step required to prepare data and build, train, and deploy models. You can quickly upload data, create new notebooks, train and tune models, move back and forth between steps to adjust experiments, compare results, and deploy models to production all in one place, making you much more productive. All ML development activities including notebooks, experiment management, automatic model creation, debugging and profiling, and model drift detection can be performed within the unified SageMaker Studio visual interface.\"What is RStudio on Amazon', 'source': 'Amazon_SageMaker_FAQs.pdf', 'chunk_index': 8}, vector=None, shard_key=None, order_value=None)]\n",
      "+++++++++++++++++++++++++++++++++++++\n",
      "id='b9341eb9-a580-4808-8052-231a26d4963c' version=0 score=0.7403473 payload={'text': 'trained. These details can help determine if a particular model input has more influence than it should on overall model behavior. SageMaker Clarify also makes explanations for individual predictions available via an API.\\xa0What is Amazon SageMaker Studio?,\"Amazon SageMaker Studio provides a single, web-based visual interface where you can perform all ML development steps. SageMaker Studio gives you complete access, control, and visibility into each step required to prepare data and build, train, and deploy models. You can quickly upload data, create new notebooks, train and tune models, move back and forth between steps to adjust experiments, compare results, and deploy models to production all in one place, making you much more productive. All ML development activities including notebooks, experiment management, automatic model creation, debugging and profiling, and model drift detection can be performed within the unified SageMaker Studio visual interface.\"What is RStudio on Amazon', 'source': 'Amazon_SageMaker_FAQs.pdf', 'chunk_index': 8} vector=None shard_key=None order_value=None\n",
      "-------------------------------\n",
      "  Retrieved chunk (score: 0.7403, source: Amazon_SageMaker_FAQs.pdf):\n",
      "    trained. These details can help determine if a particular model input has more influence than it should on overall model behavior. SageMaker Clarify a...\n",
      "id='24c2b24d-dd8f-4115-b739-a702510a235f' version=1 score=0.7403473 payload={'text': 'trained. These details can help determine if a particular model input has more influence than it should on overall model behavior. SageMaker Clarify also makes explanations for individual predictions available via an API.\\xa0What is Amazon SageMaker Studio?,\"Amazon SageMaker Studio provides a single, web-based visual interface where you can perform all ML development steps. SageMaker Studio gives you complete access, control, and visibility into each step required to prepare data and build, train, and deploy models. You can quickly upload data, create new notebooks, train and tune models, move back and forth between steps to adjust experiments, compare results, and deploy models to production all in one place, making you much more productive. All ML development activities including notebooks, experiment management, automatic model creation, debugging and profiling, and model drift detection can be performed within the unified SageMaker Studio visual interface.\"What is RStudio on Amazon', 'source': 'Amazon_SageMaker_FAQs.pdf', 'chunk_index': 8} vector=None shard_key=None order_value=None\n",
      "-------------------------------\n",
      "  Retrieved chunk (score: 0.7403, source: Amazon_SageMaker_FAQs.pdf):\n",
      "    trained. These details can help determine if a particular model input has more influence than it should on overall model behavior. SageMaker Clarify a...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/_4/v7yrfqrs7cg_p6g4z79xh24h0000gn/T/ipykernel_84373/3981225841.py:46: DeprecationWarning: `search` method is deprecated and will be removed in the future. Use `query_points` instead.\n",
      "  search_result = retrieval_qdrant_client.search(\n"
     ]
    }
   ],
   "source": [
    "query1 = \"What is Amazon EC2?\"\n",
    "retrieved_data1 = retrieve_context(query1, top_k=2)\n",
    "# The 'retrieved_data1' list will contain dictionaries with 'text' and 'source'\n",
    "\n",
    "query2 = \"How does SageMaker help with machine learning?\"\n",
    "retrieved_data2 = retrieve_context(query2, top_k=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "31908576",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'chunk_index': 474,\n",
      "  'source': 'AmazonElasticComputeCloud-DeveloperGuide-ec2-dg.pdf',\n",
      "  'text': 'NoteThere\\'smoreonGitHub.FindthecompleteexampleandlearnhowtosetupandrunintheAWSCodeExamplesRepository.#AssociatesanElasticIPaddresswithanAmazonElasticComputeCloud#(AmazonEC2)instance.##Prerequisites:##-TheallocationIDcorrespondingtotheElasticIPaddress.#-TheAmazonEC2instance.##@paramec2_client[Aws::EC2::Client]AninitializedEC2client.#@paramallocation_id[String]TheIDoftheallocationcorrespondingto#theElasticIPaddress.#@paraminstance_id[String]TheIDoftheinstance.#@return[String]TheassocationIDcorrespondingtotheassociationofthe#ElasticIPaddresstotheinstance.#@example#putsallocate_elastic_ip_address(#Aws::EC2::Client.new(region:\\'us-west-2\\'),#\\'eipalloc-04452e528a66279EX\\',#\\'i-033c48ef067af3dEX\\')defassociate_elastic_ip_address_with_instance(ec2_client,allocation_id,instance_id)response=ec2_client.associate_address(allocation_id:allocation_id,instance_id:instance_id)response.association_idrescueStandardError=>eputs\"ErrorassociatingElasticIPaddresswithinstance:#{e.message}\"\\'Error\\'Actions345Amazon'},\n",
      " {'chunk_index': 474,\n",
      "  'source': 'AmazonElasticComputeCloud-DeveloperGuide-ec2-dg.pdf',\n",
      "  'text': 'NoteThere\\'smoreonGitHub.FindthecompleteexampleandlearnhowtosetupandrunintheAWSCodeExamplesRepository.#AssociatesanElasticIPaddresswithanAmazonElasticComputeCloud#(AmazonEC2)instance.##Prerequisites:##-TheallocationIDcorrespondingtotheElasticIPaddress.#-TheAmazonEC2instance.##@paramec2_client[Aws::EC2::Client]AninitializedEC2client.#@paramallocation_id[String]TheIDoftheallocationcorrespondingto#theElasticIPaddress.#@paraminstance_id[String]TheIDoftheinstance.#@return[String]TheassocationIDcorrespondingtotheassociationofthe#ElasticIPaddresstotheinstance.#@example#putsallocate_elastic_ip_address(#Aws::EC2::Client.new(region:\\'us-west-2\\'),#\\'eipalloc-04452e528a66279EX\\',#\\'i-033c48ef067af3dEX\\')defassociate_elastic_ip_address_with_instance(ec2_client,allocation_id,instance_id)response=ec2_client.associate_address(allocation_id:allocation_id,instance_id:instance_id)response.association_idrescueStandardError=>eputs\"ErrorassociatingElasticIPaddresswithinstance:#{e.message}\"\\'Error\\'Actions345Amazon'}]\n",
      "----------------------------------------\n",
      "[{'chunk_index': 8,\n",
      "  'source': 'Amazon_SageMaker_FAQs.pdf',\n",
      "  'text': 'trained. These details can help determine if a particular model '\n",
      "          'input has more influence than it should on overall model behavior. '\n",
      "          'SageMaker Clarify also makes explanations for individual '\n",
      "          'predictions available via an API.\\xa0What is Amazon SageMaker '\n",
      "          'Studio?,\"Amazon SageMaker Studio provides a single, web-based '\n",
      "          'visual interface where you can perform all ML development steps. '\n",
      "          'SageMaker Studio gives you complete access, control, and visibility '\n",
      "          'into each step required to prepare data and build, train, and '\n",
      "          'deploy models. You can quickly upload data, create new notebooks, '\n",
      "          'train and tune models, move back and forth between steps to adjust '\n",
      "          'experiments, compare results, and deploy models to production all '\n",
      "          'in one place, making you much more productive. All ML development '\n",
      "          'activities including notebooks, experiment management, automatic '\n",
      "          'model creation, debugging and profiling, and model drift detection '\n",
      "          'can be performed within the unified SageMaker Studio visual '\n",
      "          'interface.\"What is RStudio on Amazon'},\n",
      " {'chunk_index': 8,\n",
      "  'source': 'Amazon_SageMaker_FAQs.pdf',\n",
      "  'text': 'trained. These details can help determine if a particular model '\n",
      "          'input has more influence than it should on overall model behavior. '\n",
      "          'SageMaker Clarify also makes explanations for individual '\n",
      "          'predictions available via an API.\\xa0What is Amazon SageMaker '\n",
      "          'Studio?,\"Amazon SageMaker Studio provides a single, web-based '\n",
      "          'visual interface where you can perform all ML development steps. '\n",
      "          'SageMaker Studio gives you complete access, control, and visibility '\n",
      "          'into each step required to prepare data and build, train, and '\n",
      "          'deploy models. You can quickly upload data, create new notebooks, '\n",
      "          'train and tune models, move back and forth between steps to adjust '\n",
      "          'experiments, compare results, and deploy models to production all '\n",
      "          'in one place, making you much more productive. All ML development '\n",
      "          'activities including notebooks, experiment management, automatic '\n",
      "          'model creation, debugging and profiling, and model drift detection '\n",
      "          'can be performed within the unified SageMaker Studio visual '\n",
      "          'interface.\"What is RStudio on Amazon'}]\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "pprint(retrieved_data1)\n",
    "print(\"----------------------------------------\")\n",
    "pprint(retrieved_data2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c901be25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -q  google-generativeai # If you choose Google Gemini\n",
    "\n",
    "# Placeholder for LLM API integration.\n",
    "# Replace with actual LLM client and API key.\n",
    "\n",
    "# --- Example using Google Gemini (Requires an API key) ---\n",
    "import google.generativeai as genai\n",
    "import os # To load API key from environment variables\n",
    "\n",
    "# Configure your API key\n",
    "# genai.configure(api_key=os.environ.get(\"GEMINI_API_KEY\"))\n",
    "# You'd set this environment variable in your system or use dotenv\n",
    "# import dotenv; dotenv.load_dotenv()\n",
    "# Or directly: genai.configure(api_key=\"YOUR_GEMINI_API_KEY\")\n",
    "\n",
    "# Create a GenerativeModel instance\n",
    "\n",
    "import dotenv\n",
    "dotenv.load_dotenv()\n",
    "genai.configure(api_key=os.environ.get(\"GEMINI_API_KEY\"))\n",
    "gemini_model = genai.GenerativeModel('gemini-2.0-flash-thinking-exp-1219') # Or 'gemini-1.5-pro'\n",
    "def generate_answer(query, context_chunks):\n",
    "    \"\"\"\n",
    "    Generates an answer to the query using the provided context and an LLM.\n",
    "\n",
    "    Args:\n",
    "        query (str): The user's original question.\n",
    "        context_chunks (list[dict]): A list of dictionaries, where each dict\n",
    "                                     contains 'text' and 'source' of a retrieved chunk.\n",
    "\n",
    "    Returns:\n",
    "        str: The generated answer from the LLM.\n",
    "    \"\"\"\n",
    "    if not context_chunks:\n",
    "        return \"I apologize, but I couldn't find relevant information in the documents to answer your question.\"\n",
    "\n",
    "    # --- 1. Construct the Context String ---\n",
    "    # Combine all retrieved chunk texts into a single string\n",
    "    combined_context = \"\\n\\n\".join([chunk['text'] for chunk in context_chunks])\n",
    "\n",
    "    # Optional: Add source information to the combined context for the LLM if desired\n",
    "    # or for external tracking, though for pure answer generation, just the text is fine.\n",
    "    # source_info = \"\\nSources:\\n\" + \"\\n\".join(list(set([chunk['source'] for chunk in context_chunks])))\n",
    "    # combined_context += source_info\n",
    "\n",
    "    # --- 2. Construct the LLM Prompt ---\n",
    "    # This is a critical part - clear instructions help the LLM perform better.\n",
    "    prompt = f\"\"\"\n",
    "    You are a helpful assistant that answers questions based on the provided context only.\n",
    "    If the answer cannot be found in the context, clearly state that you cannot answer from the provided information.\n",
    "\n",
    "    Question: {query}\n",
    "\n",
    "    Context:\n",
    "    {combined_context}\n",
    "\n",
    "    Answer:\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"\\n--- Sending to LLM ---\")\n",
    "    print(\"Prompt being sent (first 500 chars):\\n\", prompt[:500], \"...\")\n",
    "\n",
    "    try:\n",
    "        # --- 3. Send Prompt to LLM and Get Response (Gemini-specific) ---\n",
    "        llm_response = gemini_model.generate_content(\n",
    "            prompt,\n",
    "            # Optional: Configure generation settings\n",
    "            generation_config=genai.types.GenerationConfig(\n",
    "                temperature=0.2, # Lower temperature for more factual, less creative answers\n",
    "                max_output_tokens=1024 # Limit the length of the response\n",
    "            ),\n",
    "            # Optional: Configure safety settings if you want to relax/tighten them\n",
    "            # safety_settings={\n",
    "            #     'HARM_CATEGORY_HARASSMENT': 'BLOCK_NONE',\n",
    "            #     'HARM_CATEGORY_HATE_SPEECH': 'BLOCK_NONE',\n",
    "            #     'HARM_CATEGORY_SEXUALLY_EXPLICIT': 'BLOCK_NONE',\n",
    "            #     'HARM_CATEGORY_DANGEROUS_CONTENT': 'BLOCK_NONE',\n",
    "            # },\n",
    "        )\n",
    "        # Gemini's response object might have parts, or raise an error if it's blocked/empty\n",
    "        # Check if the response actually contains text\n",
    "        if llm_response.parts:\n",
    "            return llm_response.text\n",
    "        else:\n",
    "            # This handles cases where response might be empty or blocked by safety settings\n",
    "            print(\"Gemini response was empty or blocked.\")\n",
    "            # print(f\"Prompt feedback: {response.prompt_feedback}\") # Uncomment for debugging\n",
    "            # print(f\"Candidates: {response.candidates}\") # Uncomment for debugging\n",
    "            return \"I'm sorry, I couldn't generate an answer. The model might have blocked the response due to safety concerns or found no suitable content.\"\n",
    "\n",
    "    except genai.types.BlockedPromptException as e:\n",
    "        print(f\"Gemini API Error: Prompt was blocked by safety settings. Details: {e}\")\n",
    "        return \"I'm sorry, your request could not be processed due to safety guidelines.\"\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred while generating the answer with Gemini: {e}\")\n",
    "        return \"An error occurred while trying to generate an answer.\"\n",
    "\n",
    "# --- Implementation Instructions ---\n",
    "\n",
    "# 1. Choose your LLM:\n",
    "#    Decide which LLM API you want to use (e.g., Google Gemini, OpenAI).\n",
    "#    If using Google Gemini:\n",
    "#       - pip install -q  google-generativeai\n",
    "#       - Get your API key from Google AI Studio.\n",
    "#       - Uncomment and configure `genai.configure(api_key=...)` and `model = genai.GenerativeModel('gemini-pro')`.\n",
    "\n",
    "# 2. Integrate the LLM client:\n",
    "#    Replace the \"Placeholder for other LLM APIs\" section in the `generate_answer` function\n",
    "#    with the actual code to call your chosen LLM.\n",
    "\n",
    "# 3. Use the function with your retrieved data:\n",
    "\n",
    "#    # Example using data from your previous steps:\n",
    "#    # query1 = \"What is Amazon EC2?\"\n",
    "#    # retrieved_data1 = retrieve_context(query1, top_k=2)\n",
    "#    # final_answer1 = generate_answer(query1, retrieved_data1)\n",
    "#    # print(\"\\nFinal Answer 1:\", final_answer1)\n",
    "#\n",
    "#    # query2 = \"How does SageMaker help with machine learning?\"\n",
    "#    # retrieved_data2 = retrieve_context(query2, top_k=2)\n",
    "#    # final_answer2 = generate_answer(query2, retrieved_data2)\n",
    "#    # print(\"\\nFinal Answer 2:\", final_answer2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98d28ebc",
   "metadata": {},
   "source": [
    "## Code to query the RAG system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c13f6255",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure all previous setup cells have been run in your notebook:\n",
    "# - PDF text extraction\n",
    "# - Text chunking (into ec2_chunks, sagemaker_chunks)\n",
    "# - SentenceTransformer model loading (for embeddings)\n",
    "# - Qdrant client initialization and collection creation/upload\n",
    "# - Google Gemini API key configuration and model initialization\n",
    "# - `retrieve_context` and `generate_answer` functions defined\n",
    "\n",
    "def query_rag_system(user_query, top_k_chunks=3):\n",
    "    \"\"\"\n",
    "    Queries the RAG system to find relevant information and generate an answer.\n",
    "\n",
    "    Args:\n",
    "        user_query (str): The question the user wants to ask.\n",
    "        top_k_chunks (int): The number of top relevant chunks to retrieve from Qdrant.\n",
    "\n",
    "    Returns:\n",
    "        str: The generated answer from the LLM based on the retrieved context.\n",
    "    \"\"\"\n",
    "    print(f\"\\n--- Processing Query: '{user_query}' ---\")\n",
    "\n",
    "    # Step 1: Retrieve relevant context from Qdrant\n",
    "    print(\"Retrieving context from vector database...\")\n",
    "    retrieved_data = retrieve_context(user_query, top_k=top_k_chunks)\n",
    "\n",
    "    if not retrieved_data:\n",
    "        print(\"No relevant context found. Cannot generate an answer.\")\n",
    "        return \"I couldn't find any relevant information in my documents to answer your question.\"\n",
    "\n",
    "    # Step 2: Generate answer using the LLM with the retrieved context\n",
    "    print(\"Generating answer using the Language Model...\")\n",
    "    final_answer = generate_answer(user_query, retrieved_data)\n",
    "\n",
    "    return final_answer\n",
    "\n",
    "# --- Implementation Instructions ---\n",
    "\n",
    "# 1. Ensure ALL previous code blocks (from PDF extraction to Gemini setup)\n",
    "#    have been run successfully in your notebook.\n",
    "#    Specifically, the `retrieve_context` and `generate_answer` functions\n",
    "#    must be defined and accessible.\n",
    "\n",
    "# 2. Run the above `query_rag_system` function definition cell.\n",
    "\n",
    "# 3. Now, you can query your RAG system!\n",
    "#    Just call the function with your question:\n",
    "\n",
    "#    print(\"--- Query 1 ---\")\n",
    "#    answer1 = query_rag_system(\"What is the main purpose of Amazon EC2?\")\n",
    "#    print(\"\\nFinal Answer 1:\", answer1)\n",
    "#\n",
    "#    print(\"\\n--- Query 2 ---\")\n",
    "#    answer2 = query_rag_system(\"How does Amazon SageMaker simplify machine learning workflows?\")\n",
    "#    print(\"\\nFinal Answer 2:\", answer2)\n",
    "#\n",
    "#    print(\"\\n--- Query 3 ---\")\n",
    "#    answer3 = query_rag_system(\"What is an Elastic IP address and how is it used with EC2?\")\n",
    "#    print(\"\\nFinal Answer 3:\", answer3)\n",
    "#\n",
    "#    print(\"\\n--- Query 4 (Out of context example) ---\")\n",
    "#    answer4 = query_rag_system(\"What is the capital of France?\")\n",
    "#    print(\"\\nFinal Answer 4:\", answer4) # Expect this to say it cannot answer from context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d2858bf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Query 1 ---\n",
      "\n",
      "--- Processing Query: 'Explain Amazon SageMaker pricing?' ---\n",
      "Retrieving context from vector database...\n",
      "\n",
      "Retrieving context for query: 'Explain Amazon SageMaker pricing?'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/_4/v7yrfqrs7cg_p6g4z79xh24h0000gn/T/ipykernel_84373/3981225841.py:46: DeprecationWarning: `search` method is deprecated and will be removed in the future. Use `query_points` instead.\n",
      "  search_result = retrieval_qdrant_client.search(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved 3 chunks from Qdrant.\n",
      "Search results:[ScoredPoint(id='8b3388b0-5d91-45b6-bc4d-a3a5a833d428', version=0, score=0.729482, payload={'text': 'to or disclosure of your content. As a customer, you maintain ownership of your content, and you select which AWS services can process, store, and host your content. We do not access your content for any purpose without your consent.\"\"How am I charged for Amazon SageMaker?\",\"You pay for ML compute, storage, and data processing resources you use for hosting the notebook, training the model, performing predictions, and logging the outputs. Amazon SageMaker allows you to select the number and type of instance used for the hosted notebook, training, and model hosting. You pay only for what you use, as you use it; there are no minimum fees and no upfront commitments. See the\\xa0Amazon SageMaker pricing page\\xa0and the\\xa0Amazon SageMaker Pricing calculator\\xa0for details.\"\"How can I optimize my Amazon SageMaker costs, such as detecting and stopping idle resources in order to avoid unnecessary charges?\",\"There are several best practices you can adopt to optimize your Amazon SageMaker resource', 'source': 'Amazon_SageMaker_FAQs.pdf', 'chunk_index': 3}, vector=None, shard_key=None, order_value=None), ScoredPoint(id='0051b89e-38a3-4cff-b22d-e06f561d39e3', version=1, score=0.729482, payload={'text': 'to or disclosure of your content. As a customer, you maintain ownership of your content, and you select which AWS services can process, store, and host your content. We do not access your content for any purpose without your consent.\"\"How am I charged for Amazon SageMaker?\",\"You pay for ML compute, storage, and data processing resources you use for hosting the notebook, training the model, performing predictions, and logging the outputs. Amazon SageMaker allows you to select the number and type of instance used for the hosted notebook, training, and model hosting. You pay only for what you use, as you use it; there are no minimum fees and no upfront commitments. See the\\xa0Amazon SageMaker pricing page\\xa0and the\\xa0Amazon SageMaker Pricing calculator\\xa0for details.\"\"How can I optimize my Amazon SageMaker costs, such as detecting and stopping idle resources in order to avoid unnecessary charges?\",\"There are several best practices you can adopt to optimize your Amazon SageMaker resource', 'source': 'Amazon_SageMaker_FAQs.pdf', 'chunk_index': 3}, vector=None, shard_key=None, order_value=None), ScoredPoint(id='183211a0-8bec-4c46-98d7-d723cd0067c1', version=2, score=0.729482, payload={'text': 'to or disclosure of your content. As a customer, you maintain ownership of your content, and you select which AWS services can process, store, and host your content. We do not access your content for any purpose without your consent.\"\"How am I charged for Amazon SageMaker?\",\"You pay for ML compute, storage, and data processing resources you use for hosting the notebook, training the model, performing predictions, and logging the outputs. Amazon SageMaker allows you to select the number and type of instance used for the hosted notebook, training, and model hosting. You pay only for what you use, as you use it; there are no minimum fees and no upfront commitments. See the\\xa0Amazon SageMaker pricing page\\xa0and the\\xa0Amazon SageMaker Pricing calculator\\xa0for details.\"\"How can I optimize my Amazon SageMaker costs, such as detecting and stopping idle resources in order to avoid unnecessary charges?\",\"There are several best practices you can adopt to optimize your Amazon SageMaker resource', 'source': 'Amazon_SageMaker_FAQs.pdf', 'chunk_index': 3}, vector=None, shard_key=None, order_value=None)]\n",
      "+++++++++++++++++++++++++++++++++++++\n",
      "id='8b3388b0-5d91-45b6-bc4d-a3a5a833d428' version=0 score=0.729482 payload={'text': 'to or disclosure of your content. As a customer, you maintain ownership of your content, and you select which AWS services can process, store, and host your content. We do not access your content for any purpose without your consent.\"\"How am I charged for Amazon SageMaker?\",\"You pay for ML compute, storage, and data processing resources you use for hosting the notebook, training the model, performing predictions, and logging the outputs. Amazon SageMaker allows you to select the number and type of instance used for the hosted notebook, training, and model hosting. You pay only for what you use, as you use it; there are no minimum fees and no upfront commitments. See the\\xa0Amazon SageMaker pricing page\\xa0and the\\xa0Amazon SageMaker Pricing calculator\\xa0for details.\"\"How can I optimize my Amazon SageMaker costs, such as detecting and stopping idle resources in order to avoid unnecessary charges?\",\"There are several best practices you can adopt to optimize your Amazon SageMaker resource', 'source': 'Amazon_SageMaker_FAQs.pdf', 'chunk_index': 3} vector=None shard_key=None order_value=None\n",
      "-------------------------------\n",
      "  Retrieved chunk (score: 0.7295, source: Amazon_SageMaker_FAQs.pdf):\n",
      "    to or disclosure of your content. As a customer, you maintain ownership of your content, and you select which AWS services can process, store, and hos...\n",
      "id='0051b89e-38a3-4cff-b22d-e06f561d39e3' version=1 score=0.729482 payload={'text': 'to or disclosure of your content. As a customer, you maintain ownership of your content, and you select which AWS services can process, store, and host your content. We do not access your content for any purpose without your consent.\"\"How am I charged for Amazon SageMaker?\",\"You pay for ML compute, storage, and data processing resources you use for hosting the notebook, training the model, performing predictions, and logging the outputs. Amazon SageMaker allows you to select the number and type of instance used for the hosted notebook, training, and model hosting. You pay only for what you use, as you use it; there are no minimum fees and no upfront commitments. See the\\xa0Amazon SageMaker pricing page\\xa0and the\\xa0Amazon SageMaker Pricing calculator\\xa0for details.\"\"How can I optimize my Amazon SageMaker costs, such as detecting and stopping idle resources in order to avoid unnecessary charges?\",\"There are several best practices you can adopt to optimize your Amazon SageMaker resource', 'source': 'Amazon_SageMaker_FAQs.pdf', 'chunk_index': 3} vector=None shard_key=None order_value=None\n",
      "-------------------------------\n",
      "  Retrieved chunk (score: 0.7295, source: Amazon_SageMaker_FAQs.pdf):\n",
      "    to or disclosure of your content. As a customer, you maintain ownership of your content, and you select which AWS services can process, store, and hos...\n",
      "id='183211a0-8bec-4c46-98d7-d723cd0067c1' version=2 score=0.729482 payload={'text': 'to or disclosure of your content. As a customer, you maintain ownership of your content, and you select which AWS services can process, store, and host your content. We do not access your content for any purpose without your consent.\"\"How am I charged for Amazon SageMaker?\",\"You pay for ML compute, storage, and data processing resources you use for hosting the notebook, training the model, performing predictions, and logging the outputs. Amazon SageMaker allows you to select the number and type of instance used for the hosted notebook, training, and model hosting. You pay only for what you use, as you use it; there are no minimum fees and no upfront commitments. See the\\xa0Amazon SageMaker pricing page\\xa0and the\\xa0Amazon SageMaker Pricing calculator\\xa0for details.\"\"How can I optimize my Amazon SageMaker costs, such as detecting and stopping idle resources in order to avoid unnecessary charges?\",\"There are several best practices you can adopt to optimize your Amazon SageMaker resource', 'source': 'Amazon_SageMaker_FAQs.pdf', 'chunk_index': 3} vector=None shard_key=None order_value=None\n",
      "-------------------------------\n",
      "  Retrieved chunk (score: 0.7295, source: Amazon_SageMaker_FAQs.pdf):\n",
      "    to or disclosure of your content. As a customer, you maintain ownership of your content, and you select which AWS services can process, store, and hos...\n",
      "Generating answer using the Language Model...\n",
      "\n",
      "--- Sending to LLM ---\n",
      "Prompt being sent (first 500 chars):\n",
      " \n",
      "    You are a helpful assistant that answers questions based on the provided context only.\n",
      "    If the answer cannot be found in the context, clearly state that you cannot answer from the provided information.\n",
      "\n",
      "    Question: Explain Amazon SageMaker pricing?\n",
      "\n",
      "    Context:\n",
      "    to or disclosure of your content. As a customer, you maintain ownership of your content, and you select which AWS services can process, store, and host your content. We do not access your content for any purpose without you ...\n",
      "\n",
      "Final Answer 1: Based on the provided context, you are charged for Amazon SageMaker for the ML compute, storage, and data processing resources you use for hosting the notebook, training the model, performing predictions, and logging the outputs. You can select the number and type of instance used for the hosted notebook, training, and model hosting. You pay only for what you use, as you use it, with no minimum fees and no upfront commitments.\n"
     ]
    }
   ],
   "source": [
    "print(\"--- Query 1 ---\")\n",
    "answer1 = query_rag_system(\"Explain Amazon SageMaker pricing?\")\n",
    "print(\"\\nFinal Answer 1:\", answer1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1b2d8db9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Listing available Gemini models:\n",
      "  Model name: models/gemini-1.0-pro-vision-latest, Supported methods: ['generateContent', 'countTokens']\n",
      "  Model name: models/gemini-pro-vision, Supported methods: ['generateContent', 'countTokens']\n",
      "  Model name: models/gemini-1.5-pro-latest, Supported methods: ['generateContent', 'countTokens']\n",
      "  Model name: models/gemini-1.5-pro-001, Supported methods: ['generateContent', 'countTokens', 'createCachedContent']\n",
      "  Model name: models/gemini-1.5-pro-002, Supported methods: ['generateContent', 'countTokens', 'createCachedContent']\n",
      "  Model name: models/gemini-1.5-pro, Supported methods: ['generateContent', 'countTokens']\n",
      "  Model name: models/gemini-1.5-flash-latest, Supported methods: ['generateContent', 'countTokens']\n",
      "  Model name: models/gemini-1.5-flash-001, Supported methods: ['generateContent', 'countTokens', 'createCachedContent']\n",
      "  Model name: models/gemini-1.5-flash-001-tuning, Supported methods: ['generateContent', 'countTokens', 'createTunedModel']\n",
      "  Model name: models/gemini-1.5-flash, Supported methods: ['generateContent', 'countTokens']\n",
      "  Model name: models/gemini-1.5-flash-002, Supported methods: ['generateContent', 'countTokens', 'createCachedContent']\n",
      "  Model name: models/gemini-1.5-flash-8b, Supported methods: ['createCachedContent', 'generateContent', 'countTokens']\n",
      "  Model name: models/gemini-1.5-flash-8b-001, Supported methods: ['createCachedContent', 'generateContent', 'countTokens']\n",
      "  Model name: models/gemini-1.5-flash-8b-latest, Supported methods: ['createCachedContent', 'generateContent', 'countTokens']\n",
      "  Model name: models/gemini-1.5-flash-8b-exp-0827, Supported methods: ['generateContent', 'countTokens']\n",
      "  Model name: models/gemini-1.5-flash-8b-exp-0924, Supported methods: ['generateContent', 'countTokens']\n",
      "  Model name: models/gemini-2.5-pro-exp-03-25, Supported methods: ['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent']\n",
      "  Model name: models/gemini-2.5-pro-preview-03-25, Supported methods: ['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent']\n",
      "  Model name: models/gemini-2.5-flash-preview-04-17, Supported methods: ['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent']\n",
      "  Model name: models/gemini-2.5-flash-preview-05-20, Supported methods: ['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent']\n",
      "  Model name: models/gemini-2.5-flash-preview-04-17-thinking, Supported methods: ['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent']\n",
      "  Model name: models/gemini-2.5-pro-preview-05-06, Supported methods: ['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent']\n",
      "  Model name: models/gemini-2.0-flash-exp, Supported methods: ['generateContent', 'countTokens', 'bidiGenerateContent']\n",
      "  Model name: models/gemini-2.0-flash, Supported methods: ['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent']\n",
      "  Model name: models/gemini-2.0-flash-001, Supported methods: ['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent']\n",
      "  Model name: models/gemini-2.0-flash-exp-image-generation, Supported methods: ['generateContent', 'countTokens', 'bidiGenerateContent']\n",
      "  Model name: models/gemini-2.0-flash-lite-001, Supported methods: ['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent']\n",
      "  Model name: models/gemini-2.0-flash-lite, Supported methods: ['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent']\n",
      "  Model name: models/gemini-2.0-flash-preview-image-generation, Supported methods: ['generateContent', 'countTokens']\n",
      "  Model name: models/gemini-2.0-flash-lite-preview-02-05, Supported methods: ['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent']\n",
      "  Model name: models/gemini-2.0-flash-lite-preview, Supported methods: ['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent']\n",
      "  Model name: models/gemini-2.0-pro-exp, Supported methods: ['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent']\n",
      "  Model name: models/gemini-2.0-pro-exp-02-05, Supported methods: ['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent']\n",
      "  Model name: models/gemini-exp-1206, Supported methods: ['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent']\n",
      "  Model name: models/gemini-2.0-flash-thinking-exp-01-21, Supported methods: ['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent']\n",
      "  Model name: models/gemini-2.0-flash-thinking-exp, Supported methods: ['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent']\n",
      "  Model name: models/gemini-2.0-flash-thinking-exp-1219, Supported methods: ['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent']\n",
      "  Model name: models/gemini-2.5-flash-preview-tts, Supported methods: ['countTokens', 'generateContent']\n",
      "  Model name: models/gemini-2.5-pro-preview-tts, Supported methods: ['countTokens', 'generateContent']\n",
      "  Model name: models/learnlm-2.0-flash-experimental, Supported methods: ['generateContent', 'countTokens']\n",
      "  Model name: models/gemma-3-1b-it, Supported methods: ['generateContent', 'countTokens']\n",
      "  Model name: models/gemma-3-4b-it, Supported methods: ['generateContent', 'countTokens']\n",
      "  Model name: models/gemma-3-12b-it, Supported methods: ['generateContent', 'countTokens']\n",
      "  Model name: models/gemma-3-27b-it, Supported methods: ['generateContent', 'countTokens']\n",
      "  Model name: models/gemma-3n-e4b-it, Supported methods: ['generateContent', 'countTokens']\n"
     ]
    }
   ],
   "source": [
    "import google.generativeai as genai\n",
    "import os\n",
    "import dotenv\n",
    "\n",
    "# Ensure your API key is configured (same as before)\n",
    "dotenv.load_dotenv()\n",
    "genai.configure(api_key=os.environ.get(\"GEMINI_API_KEY\"))\n",
    "\n",
    "print(\"Listing available Gemini models:\")\n",
    "for m in genai.list_models():\n",
    "    # Only print models that support the 'generateContent' method, as that's what we need\n",
    "    if 'generateContent' in m.supported_generation_methods:\n",
    "        print(f\"  Model name: {m.name}, Supported methods: {m.supported_generation_methods}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
